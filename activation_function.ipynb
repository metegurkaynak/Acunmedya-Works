{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5034128e-85d3-49e9-a791-7419c9738ff9",
   "metadata": {},
   "source": [
    "# Aktivasyon Fonksiyonu Nedir?\n",
    "\n",
    "Aktivasyon fonksiyonu, yapay sinir ağlarında her bir nöronun çıktısını belirleyen matematiksel bir işlemdir. Girdilere uygulanan ağırlıklar ve bias ile elde edilen toplam değer, bu fonksiyondan geçirilerek nöronun son çıktısı elde edilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dabadff-696f-490a-97e8-de5a46d72d74",
   "metadata": {},
   "source": [
    "## Ne İşe Yarar?\n",
    "\n",
    "- Doğrusal olmayanlık kazandırır: Sinir ağları, aktivasyon fonksiyonları sayesinde doğrusal olmayan problemleri öğrenebilir. Bu sayede karmaşık ilişkileri modelleyebilir.\n",
    "- Çıktıyı sınırlar: Fonksiyon, nöron çıktısını belli bir aralığa (örneğin 0-1 veya -1 ile 1 arası) sınırlandırabilir.\n",
    "- Ağ derinliğini kullanabilir hale getirir: Katman sayısı arttıkça, aktivasyon fonksiyonları sayesinde model daha derin ve güçlü hale gelir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59902bee-3f0b-4853-a76e-199b8b2dcdb6",
   "metadata": {},
   "source": [
    "# Aktivasyon Fonksiyonu Türleri\n",
    "\n",
    "## 1. **Sigmoid Fonksiyonu**\n",
    "\n",
    "\n",
    "\n",
    "- Çıktı aralığı: (0, 1)\n",
    "- Avantaj: Olasılık yorumlarına uygundur.\n",
    "- Dezavantaj: Vanishing gradient (kaybolan gradyan) problemleri yaratabilir.\n",
    "\n",
    "## 2. **Tanh (Hiperbolik Tanjant) Fonksiyonu**\n",
    "\n",
    "\n",
    "\n",
    "- Çıktı aralığı: (-1, 1)\n",
    "- Sigmoid'e göre daha güçlüdür çünkü çıktının ortalaması sıfıra daha yakındır.\n",
    "\n",
    "## 3. **ReLU (Rectified Linear Unit)**\n",
    "\n",
    "\n",
    "\n",
    "- Çıktı aralığı: [0, ∞)\n",
    "- Avantaj: Hesaplama açısından hızlıdır ve vanishing gradient problemi daha az görülür.\n",
    "- Dezavantaj: Negatif girdilerde gradyan sıfır olduğu için bazı nöronlar tamamen etkisiz hale gelebilir (dying ReLU problemi).\n",
    "\n",
    "## 4. **Leaky ReLU**\n",
    "\n",
    "\n",
    "\n",
    "- Negatif değerler için küçük bir eğim bırakır (\\(\\alpha\\) genellikle 0.01).\n",
    "- Dying ReLU problemini azaltır.\n",
    "\n",
    "## 5. **Softmax Fonksiyonu**\n",
    "\n",
    "\n",
    "\n",
    "- Genellikle çok sınıflı sınıflandırma problemlerinde, son katmanda kullanılır.\n",
    "- Çıktılar, her sınıfa ait olasılıkları ifade eder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791817b2-1a1b-4377-8ecb-8737fcb0ad95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

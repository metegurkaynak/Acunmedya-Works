{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a69040b3-a6f5-4090-a32b-801cbe4ac1bd",
   "metadata": {},
   "source": [
    "## Backpropagation Nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d940418a-1cca-4514-aee3-bea50573fb90",
   "metadata": {},
   "source": [
    "Backpropagation (geri yayılım), yapay sinir ağlarının öğrenme sürecinde kullanılan temel algoritmadır. Hedef; modelin yaptığı hatayı (loss) minimize edecek şekilde ağırlıkları güncellemektir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb0306f-367c-45ae-9edd-04436686981f",
   "metadata": {},
   "source": [
    "### Nasıl Çalışır?\n",
    "\n",
    "1. İleri Yayılım (Forward Pass): Girdilerden başlanarak, ağ boyunca çıktıya kadar hesaplama yapılır.\n",
    "2. Kayıp (Loss) Hesaplama: Gerçek değer ile tahmin arasındaki fark hesaplanır.\n",
    "3. Geri Yayılım: Zincir kuralı kullanılarak hata, ağın katmanları boyunca geriye doğru yayılır.\n",
    "4. Ağırlık Güncelleme: Hatalar kullanılarak her ağırlık, öğrenme oranı (learning rate) doğrultusunda güncellenir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e8e36e-856b-47f0-bbfe-b4a4d41caa9c",
   "metadata": {},
   "source": [
    "## Optimizer (Optimizasyon Algoritması) Nedir?\n",
    "\n",
    "Optimizer’lar, sinir ağı eğitiminde ağırlıkların nasıl güncelleneceğini belirleyen algoritmalardır. Amaç, kayıp fonksiyonunu minimize etmektir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27996a9b-65cc-489a-93a6-1ac4f4c2c4fa",
   "metadata": {},
   "source": [
    "## Optimizer Türleri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6c077a-246a-4fff-adfc-6bf2b2d635c2",
   "metadata": {},
   "source": [
    "### 1. **Gradient Descent (GD)**\n",
    "\n",
    "- Tüm veri üzerinden gradyan hesaplar.\n",
    "- Çok yavaş olabilir, büyük veri setlerinde verimsizdir.\n",
    "\n",
    "### 2. **Stochastic Gradient Descent (SGD)**\n",
    "\n",
    "- Her iterasyonda tek bir örnek üzerinden ağırlık günceller.\n",
    "- Daha hızlıdır ama daha gürültülü sonuçlar verebilir.\n",
    "\n",
    "### 3. **Mini-Batch Gradient Descent**\n",
    "\n",
    "- Veriyi küçük parçalara (batch) bölerek her adımda gradyan hesaplar.\n",
    "- Genellikle en dengeli ve pratik yöntemdir.\n",
    "\n",
    "### 4. **Momentum**\n",
    "\n",
    "- Gradyanların yönünü takip eder, ivme kazandırır.\n",
    "- Öğrenme sürecini hızlandırır ve yerel minimumlarda takılmayı azaltır.\n",
    "\n",
    "### 5. **RMSProp**\n",
    "\n",
    "- Öğrenme oranını adaptif olarak ayarlar.\n",
    "- Özellikle tekrarlı verilerde (örneğin zaman serileri) başarılıdır.\n",
    "\n",
    "### 6. **Adam (Adaptive Moment Estimation)**\n",
    "\n",
    "- Momentum + RMSProp kombinasyonudur.\n",
    "- En yaygın kullanılan optimizer'lardandır.\n",
    "- Otomatik olarak öğrenme oranlarını ayarlar, genellikle hızlı ve etkilidir."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
